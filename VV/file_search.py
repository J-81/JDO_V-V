# Performs first layer of V&V: File existence checks
from pathlib import Path
import re

import yaml
import pandas as pd

def load_meta(runsheet: Path):
    df = pd.read_csv(runsheet)
    df = df.set_index("sample_name") 
    return df.to_dict(orient="index")

def get_listing(root_dir: Path) -> pd.DataFrame:
    """ For a given root directory, creates a dataframe of all file handles """
    listings = list()
    def _handle(handle: Path):
        """ Returns the necessary data about the file handle if the handle is for a file.  Otherwise recursivel iterate """
        if handle.is_file():
            listings.append({'fullPath':str(handle),'relativePath':str(handle.relative_to(root_dir)),'pathObj':handle,'isDir':False, 'annotations':dict()})
        elif handle.is_dir():
            listings.append({'fullPath':str(handle),'relativePath':str(handle.relative_to(root_dir)),'pathObj':handle,'isDir':True, 'annotations':dict()})
            for dir in handle.iterdir():
                _handle(dir)
    assert root_dir.is_dir()
    _handle(root_dir)
    return pd.DataFrame(listings)

def path_annotate(config_fs_f: Path, runsheet: Path, template: str, root_dir: Path, expand_annotations: bool = True):
    """ Loads file search configuration file and runsheet then perform files existence validation 

    config_fs_f: yaml file with file patterns to search for
    runsheet: csv file to pull metadata from (as generated by the AST scripts)
    template: a string denoting the platrom type (must match the two topmost keys in the yaml file pattern config), e.g. 'affymetrix:one_channel'
    root_dir: top level directory for the dataset
    """

    with open(config_fs_f, "r") as f:
        config_fs =  yaml.safe_load(f)

    # extract only the relevant config
    template_manufact, template_channels = template.split(':')
    config_fs = config_fs[template_manufact][template_channels]
    print(config_fs)

    metas = load_meta(runsheet)

    # get file and directory dataframe
    df = get_listing(root_dir)

    # iterate through listing and annotate based on config
    file_mapping = dict()
    for sample_name, meta in metas.items():
        print(sample_name)
        # search for each file pattern
        for file_type, config_specific  in config_fs.items():
            print(f"Searching for type: {file_type}")
            pattern = config_specific["pattern"]
            annotations = config_specific["annotations"]
            search_path = pattern.format(sample_name=sample_name,
                                         sample=sample_name,
                                         source_name=meta.get('Source Name'), 
                                         hybridization_assay=meta.get('Hybridization Assay Name') )
            annotations['search_pattern'] = search_path
            # find matches
            try:
                df_matches = df.loc[df['relativePath'].str.match(search_path)]
            except re.error as e:
                raise ValueError(f"Bad re_pattern. Failing pattern: {search_path}")
            print(f"Found {len(df_matches)} matches with pattern '{search_path}'")
            # set annotations from config for all matching cases
            if len(df_matches):
                df.loc[df['relativePath'].str.match(search_path), 'annotations'] = [annotations for _ in range(len(df_matches))] # assign the annotation with a list of appropriate length
            elif config_specific.get("optional"):
                print(f"Note: no matches for {file_type}")
            else:
                raise ValueError(f"No files matched when file type {file_type} set as NOT optional")
    # add default annotation for all other files
    df.loc[df['annotations'] == {}, 'annotations'] = [config_fs['DEFAULT']['annotations'] for _ in range(len(df.loc[df['annotations'] == {}]))] # assign the annotation with a list of appropriate length

    if expand_annotations:
        # expand annotations into their own columns
        df_annot = pd.json_normalize(df['annotations'])
        # concat columns
        df = pd.concat([df, df_annot], axis='columns')
    return df
            
                
